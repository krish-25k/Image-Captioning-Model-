{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Project\n",
    "## Using CNN, RNN, YOLO and Transformers with Real Dataset\n",
    "\n",
    "**Important:** After running the installation cell (Cell 1), you may need to **restart the kernel** for the packages to be properly loaded. Go to: Kernel â†’ Restart Kernel, then run all cells again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries with proper version constraints\n",
    "%pip install \"numpy<2\" \"torch>=2.0\" torchvision transformers pillow requests nltk datasets accelerate ipywidgets jupyter ultralytics opencv-python -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install \"openai>=1.40.0,<2.0.0\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Vocabulary and CNN-RNN Model Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for Vocabulary\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary class for word-to-index mapping\"\"\"\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.word_freq = Counter()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return word_tokenize(text.lower())\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \"\"\"Build vocabulary from list of sentences\"\"\"\n",
    "        for sentence in sentence_list:\n",
    "            words = self.tokenize(sentence)\n",
    "            self.word_freq.update(words)\n",
    "        \n",
    "        # Add words that meet frequency threshold\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = len(self.itos)\n",
    "                    self.itos[len(self.itos)] = word\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"Convert text to numerical tokens\"\"\"\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def textify(self, numerical_list):\n",
    "        \"\"\"Convert numerical tokens back to text\"\"\"\n",
    "        return [self.itos[idx] for idx in numerical_list if idx not in [0, 1, 2, 3]]\n",
    "\n",
    "print(\"Vocabulary class ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for CNN-RNN model components\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism for image-caption alignment\"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.A = nn.Linear(attention_dim, 1)\n",
    "    \n",
    "    def forward(self, features, decoder_hidden):\n",
    "        u_hs = self.U(features)\n",
    "        w_h = self.W(decoder_hidden).unsqueeze(1)\n",
    "        att = self.A(torch.tanh(u_hs + w_h))\n",
    "        alpha = F.softmax(att.squeeze(2), dim=1)\n",
    "        attention_weighted_encoding = (features * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\"CNN Encoder supporting multiple torchvision backbones\"\"\"\n",
    "    def __init__(self, encoder_name='resnet50', encoded_image_size=14, fine_tune=True):\n",
    "        super().__init__()\n",
    "        self.encoder_name = encoder_name.lower()\n",
    "        self.encoder_dim = None\n",
    "        self.backbone = self._build_backbone(self.encoder_name)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune(fine_tune)\n",
    "    \n",
    "    def _build_backbone(self, encoder_name):\n",
    "        if encoder_name == 'resnet50':\n",
    "            backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.encoder_dim = 2048\n",
    "            return nn.Sequential(*list(backbone.children())[:-2])\n",
    "        elif encoder_name == 'efficientnet_b3':\n",
    "            backbone = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "            self.encoder_dim = backbone.classifier[1].in_features\n",
    "            return backbone.features\n",
    "        elif encoder_name == 'convnext_small':\n",
    "            backbone = models.convnext_small(weights=models.ConvNeXt_Small_Weights.IMAGENET1K_V1)\n",
    "            self.encoder_dim = 768\n",
    "            return backbone.features\n",
    "        else:\n",
    "            # Default to resnet50\n",
    "            backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.encoder_dim = 2048\n",
    "            return nn.Sequential(*list(backbone.children())[:-2])\n",
    "    \n",
    "    def forward(self, images):\n",
    "        out = self.backbone(images)\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        batch_size = out.size(0)\n",
    "        return out.view(batch_size, -1, self.encoder_dim)\n",
    "    \n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        if fine_tune:\n",
    "            for layer in list(self.backbone.children())[-3:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    \"\"\"RNN Decoder with Attention supporting LSTM or GRU cells\"\"\"\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048,\n",
    "                 dropout=0.5, decoder_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout\n",
    "        self.decoder_type = decoder_type.lower()\n",
    "        \n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        rnn_input_dim = embed_dim + encoder_dim\n",
    "        if self.decoder_type == 'gru':\n",
    "            self.decode_cell = nn.GRUCell(rnn_input_dim, decoder_dim, bias=True)\n",
    "        else:\n",
    "            self.decode_cell = nn.LSTMCell(rnn_input_dim, decoder_dim, bias=True)\n",
    "            self.decoder_type = 'lstm'\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        if self.decoder_type == 'lstm':\n",
    "            c = self.init_c(mean_encoder_out)\n",
    "        else:\n",
    "            c = torch.zeros_like(h)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        max_decode_len = max(decode_lengths)\n",
    "        vocab_size = self.vocab_size\n",
    "        predictions = torch.zeros(batch_size, max_decode_len, vocab_size, device=encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, max_decode_len, num_pixels, device=encoder_out.device)\n",
    "        \n",
    "        for t in range(max_decode_len):\n",
    "            batch_size_t = sum(l > t for l in decode_lengths)\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = torch.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            rnn_input = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
    "            if self.decoder_type == 'lstm':\n",
    "                h_t, c_t = self.decode_cell(rnn_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "                h = h.clone()\n",
    "                c = c.clone()\n",
    "                h[:batch_size_t], c[:batch_size_t] = h_t, c_t\n",
    "            else:\n",
    "                h_t = self.decode_cell(rnn_input, h[:batch_size_t])\n",
    "                h = h.clone()\n",
    "                h[:batch_size_t] = h_t\n",
    "            preds = self.fc(self.dropout(h[:batch_size_t]))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
    "\n",
    "\n",
    "class ImageCaptionModel(nn.Module):\n",
    "    \"\"\"Complete Image Captioning Model with configurable encoder/decoder\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=256, decoder_dim=512, attention_dim=256,\n",
    "                 encoder_name='resnet50', dropout=0.5, fine_tune_encoder=True, decoder_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(encoder_name=encoder_name, fine_tune=fine_tune_encoder)\n",
    "        self.decoder = RNNDecoder(attention_dim, embed_dim, decoder_dim, vocab_size,\n",
    "                                  encoder_dim=self.encoder.encoder_dim, dropout=dropout,\n",
    "                                  decoder_type=decoder_type)\n",
    "    \n",
    "    def forward(self, images, encoded_captions, caption_lengths):\n",
    "        encoder_out = self.encoder(images)\n",
    "        return self.decoder(encoder_out, encoded_captions, caption_lengths)\n",
    "\n",
    "print(\"CNN-RNN model components ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import YOLO and check availability\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: YOLO not available. Install with: pip install ultralytics\")\n",
    "    YOLO_AVAILABLE = False\n",
    "    YOLO = None\n",
    "\n",
    "class YOLOObjectDetector:\n",
    "    \"\"\"YOLO-based object detector for enhancing caption context\"\"\"\n",
    "    def __init__(self, model_name='yolov8n.pt'):\n",
    "        self.model = None\n",
    "        if YOLO_AVAILABLE:\n",
    "            try:\n",
    "                self.model = YOLO(model_name)\n",
    "                print(f\"YOLO model {model_name} loaded successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load YOLO model: {e}\")\n",
    "                self.model = None\n",
    "    \n",
    "    def detect_objects(self, image):\n",
    "        \"\"\"Detect objects in image and return list of object names\"\"\"\n",
    "        if self.model is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            results = self.model(image, verbose=False)\n",
    "            objects = []\n",
    "            for result in results:\n",
    "                if result.boxes is not None:\n",
    "                    for box in result.boxes:\n",
    "                        class_id = int(box.cls[0])\n",
    "                        class_name = self.model.names[class_id]\n",
    "                        confidence = float(box.conf[0])\n",
    "                        if confidence > 0.5:  # Only high confidence detections\n",
    "                            objects.append(class_name)\n",
    "            return list(set(objects))  # Return unique objects\n",
    "        except Exception as e:\n",
    "            print(f\"Error in object detection: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize YOLO detector\n",
    "yolo_detector = YOLOObjectDetector()\n",
    "print(\"YOLO integration ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. CNN-RNN Dataset for Hugging Face Flickr8k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for CNNRNNDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CNNRNNDataset(Dataset):\n",
    "    \"\"\"Dataset for CNN-RNN model using Hugging Face Flickr8k dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, vocab, max_length=50, transform=None, split='train', use_yolo=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: Hugging Face dataset (DatasetDict or Dataset)\n",
    "            vocab: Vocabulary object\n",
    "            max_length: Maximum caption length\n",
    "            transform: Optional image transforms\n",
    "            split: Dataset split to use ('train', 'validation', 'test')\n",
    "            use_yolo: Whether to use YOLO for object detection enhancement\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        self.use_yolo = use_yolo\n",
    "        \n",
    "        # Default transform for CNN-RNN\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        \n",
    "        # Extract the split from DatasetDict or use the dataset directly\n",
    "        if hasattr(hf_dataset, 'keys'):\n",
    "            if split in hf_dataset:\n",
    "                self.dataset = hf_dataset[split]\n",
    "            else:\n",
    "                raise ValueError(f\"Split '{split}' not found in dataset. Available splits: {list(hf_dataset.keys())}\")\n",
    "        else:\n",
    "            self.dataset = hf_dataset\n",
    "        \n",
    "        print(f\"Loaded {len(self.dataset)} examples from {split} split for CNN-RNN\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Get image (already a PIL Image from Hugging Face)\n",
    "        image = example['image']\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Get caption - Flickr8k has caption_0, caption_1, etc.\n",
    "        caption = example.get('caption_0', example.get('caption', 'a photo'))\n",
    "        \n",
    "        # Use YOLO to enhance context if enabled\n",
    "        if self.use_yolo:\n",
    "            try:\n",
    "                if 'yolo_detector' in globals() and yolo_detector.model is not None:\n",
    "                    objects = yolo_detector.detect_objects(image)\n",
    "                    if objects:\n",
    "                        caption = f\"{caption} {' '.join(objects[:2])}\"\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Numericalize caption\n",
    "        caption_tokens = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(caption_tokens) > self.max_length:\n",
    "            caption_tokens = caption_tokens[:self.max_length]\n",
    "        else:\n",
    "            caption_tokens = caption_tokens + [self.vocab.stoi[\"<PAD>\"]] * (self.max_length - len(caption_tokens))\n",
    "        \n",
    "        # Get image ID\n",
    "        image_id = example.get('image_id', example.get('id', f'img_{idx}'))\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'caption': torch.tensor(caption_tokens, dtype=torch.long),\n",
    "            'caption_length': torch.tensor([len([t for t in caption_tokens if t != self.vocab.stoi[\"<PAD>\"]])], dtype=torch.long),\n",
    "            'caption_text': caption,\n",
    "            'image_id': str(image_id)\n",
    "        }\n",
    "\n",
    "\n",
    "def cnn_rnn_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for CNN-RNN dataset\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    captions = torch.stack([item['caption'] for item in batch])\n",
    "    caption_lengths = torch.stack([item['caption_length'] for item in batch])\n",
    "    caption_texts = [item['caption_text'] for item in batch]\n",
    "    image_ids = [item.get('image_id', '') for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'images': images,\n",
    "        'captions': captions,\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'caption_texts': caption_texts,\n",
    "        'image_ids': image_ids\n",
    "    }\n",
    "\n",
    "print(\"CNN-RNN dataset class ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. CNN-RNN Caption Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for caption generation\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "DEFAULT_BEAM_SIZE = 5\n",
    "\n",
    "def generate_caption_beam_search(model, image, vocab, device, beam_size=None, max_length=50, repetition_penalty=1.5):\n",
    "    \"\"\"Generate caption using beam search for CNN-RNN model with repetition penalty\"\"\"\n",
    "    if beam_size is None:\n",
    "        beam_size = DEFAULT_BEAM_SIZE\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode image\n",
    "    encoder_out = model.encoder(image)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(2)\n",
    "    \n",
    "    # Initialize decoder\n",
    "    h, c = model.decoder.init_hidden_state(encoder_out)\n",
    "    \n",
    "    # Expand for beam search\n",
    "    encoder_out = encoder_out.expand(beam_size, num_pixels, encoder_dim)\n",
    "    h = h.expand(beam_size, h.size(1))\n",
    "    c = c.expand(beam_size, c.size(1))\n",
    "    \n",
    "    # Start with <SOS> token\n",
    "    k_prev_words = torch.LongTensor([[vocab.stoi[\"<SOS>\"]]] * beam_size).to(device)\n",
    "    seqs = k_prev_words\n",
    "    top_k_scores = torch.zeros(beam_size, 1).to(device)\n",
    "    \n",
    "    complete_seqs = []\n",
    "    complete_seqs_scores = []\n",
    "    \n",
    "    step = 1\n",
    "    while True:\n",
    "        embeddings = model.decoder.embedding(k_prev_words).squeeze(1)\n",
    "        attention_weighted_encoding, alpha = model.decoder.attention(encoder_out, h)\n",
    "        gate = torch.sigmoid(model.decoder.f_beta(h))\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "        rnn_input = torch.cat([embeddings, attention_weighted_encoding], dim=1)\n",
    "        if model.decoder.decoder_type == 'lstm':\n",
    "            h, c = model.decoder.decode_cell(rnn_input, (h, c))\n",
    "        else:\n",
    "            h = model.decoder.decode_cell(rnn_input, h)\n",
    "        scores = model.decoder.fc(model.decoder.dropout(h))\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "        \n",
    "        # Apply repetition penalty: penalize words that have appeared in the sequence\n",
    "        if step > 1:\n",
    "            for i in range(beam_size):\n",
    "                # Get unique words in the sequence (excluding SOS, EOS, PAD, UNK)\n",
    "                unique_words = set()\n",
    "                for word_idx in seqs[i].cpu().tolist():\n",
    "                    if word_idx not in [vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"], vocab.stoi[\"<PAD>\"], vocab.stoi[\"<UNK>\"]]:\n",
    "                        unique_words.add(word_idx)\n",
    "                \n",
    "                # Penalize repeated words\n",
    "                for word_idx in unique_words:\n",
    "                    if word_idx < scores.size(1):\n",
    "                        # Reduce the score for words that have already appeared\n",
    "                        scores[i, word_idx] = scores[i, word_idx] / repetition_penalty\n",
    "        \n",
    "        scores = top_k_scores.expand_as(scores) + scores\n",
    "        \n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(beam_size, 0, True, True)\n",
    "        else:\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(beam_size, 0, True, True)\n",
    "        \n",
    "        prev_word_inds = top_k_words // vocab.__len__()\n",
    "        next_word_inds = top_k_words % vocab.__len__()\n",
    "        \n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != vocab.stoi[\"<EOS>\"]]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "        \n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            beam_size -= len(complete_inds)\n",
    "        \n",
    "        if beam_size == 0:\n",
    "            break\n",
    "        \n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "        \n",
    "        if step > max_length:\n",
    "            break\n",
    "        step += 1\n",
    "    \n",
    "    if len(complete_seqs_scores) > 0:\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "    else:\n",
    "        seq = seqs[0].tolist()\n",
    "    \n",
    "    # Convert to text and remove consecutive duplicates\n",
    "    words = vocab.textify(seq)\n",
    "    # Remove consecutive duplicate words\n",
    "    cleaned_words = []\n",
    "    prev_word = None\n",
    "    for word in words:\n",
    "        if word != prev_word:\n",
    "            cleaned_words.append(word)\n",
    "        prev_word = word\n",
    "    \n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "\n",
    "def generate_cnn_rnn_caption(image_path_or_url, cnn_rnn_model, vocab, device, use_yolo=False, beam_size=None):\n",
    "    \"\"\"Generate caption using CNN-RNN model\"\"\"\n",
    "    try:\n",
    "        beam_size = beam_size or DEFAULT_BEAM_SIZE\n",
    "        # Load image\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            image_pil = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            image_pil = Image.open(image_path_or_url).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        image_tensor = transform(image_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use YOLO to detect objects if enabled\n",
    "        detected_objects = []\n",
    "        if use_yolo and 'yolo_detector' in globals() and yolo_detector.model is not None:\n",
    "            try:\n",
    "                detected_objects = yolo_detector.detect_objects(image_pil)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Generate caption\n",
    "        cnn_rnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            caption = generate_caption_beam_search(\n",
    "                cnn_rnn_model, image_tensor, vocab, device, beam_size=beam_size, max_length=50\n",
    "            )\n",
    "        \n",
    "        # Enhance caption with YOLO objects if detected\n",
    "        if detected_objects and use_yolo:\n",
    "            objects_str = \", \".join(detected_objects[:3])\n",
    "            caption = f\"{caption} (contains: {objects_str})\"\n",
    "        \n",
    "        return image_pil, caption\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return None, f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n",
    "\n",
    "\n",
    "def generate_transformer_caption_from_pil(image_pil, transformer_model, processor, device):\n",
    "    \"\"\"Generate caption using transformer model from PIL image\"\"\"\n",
    "    try:\n",
    "        inputs = processor(images=image_pil, return_tensors=\"pt\").to(device)\n",
    "        transformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = transformer_model.generate(\n",
    "                **inputs,\n",
    "                max_length=50,\n",
    "                num_beams=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"(transformer error: {e})\"\n",
    "\n",
    "print(\"CNN-RNN caption generation functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Image-based Recommendation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules for recommendations\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Global variables for image-based recommendations\n",
    "_last_uploaded_image = None\n",
    "_last_cnn_caption = None\n",
    "_last_transformer_caption = None\n",
    "_last_yolo_objects = None\n",
    "\n",
    "def get_image_based_recommendations(base_text, image_pil=None, cnn_caption=None, transformer_caption=None, yolo_objects=None, vocab=None):\n",
    "    \"\"\"Get word recommendations based on the uploaded image and generated captions.\"\"\"\n",
    "    words = set()\n",
    "    base_tokens = set(word_tokenize(base_text.lower())) if base_text else set()\n",
    "    \n",
    "    # 1. Add words from generated captions (most relevant)\n",
    "    for cap in [cnn_caption, transformer_caption]:\n",
    "        if cap:\n",
    "            for w in word_tokenize(cap.lower()):\n",
    "                if w.isalpha() and len(w) > 2 and w not in base_tokens:\n",
    "                    words.add(w)\n",
    "    \n",
    "    # 2. Add YOLO-detected objects (image-specific context)\n",
    "    if yolo_objects:\n",
    "        for obj in yolo_objects:\n",
    "            obj_words = word_tokenize(obj.lower())\n",
    "            for w in obj_words:\n",
    "                if w.isalpha() and len(w) > 2 and w not in base_tokens:\n",
    "                    words.add(w)\n",
    "    \n",
    "    # 3. Add common caption words from vocabulary (if available)\n",
    "    if vocab is not None:\n",
    "        common_caption_words = ['man', 'woman', 'person', 'people', 'standing', 'sitting', 'walking', \n",
    "                               'wearing', 'holding', 'looking', 'smiling', 'playing', 'running',\n",
    "                               'dog', 'cat', 'car', 'building', 'tree', 'sky', 'water', 'beach',\n",
    "                               'mountain', 'street', 'park', 'indoor', 'outdoor', 'young', 'old',\n",
    "                               'red', 'blue', 'green', 'white', 'black', 'yellow', 'large', 'small']\n",
    "        for w in common_caption_words:\n",
    "            if w not in base_tokens and len(w) > 2:\n",
    "                words.add(w)\n",
    "    \n",
    "    # 4. Add top vocabulary words (fallback)\n",
    "    if vocab is not None and len(words) < 10:\n",
    "        for w in list(getattr(vocab, 'word_freq', {}).keys())[:100]:\n",
    "            if w not in base_tokens and len(w) > 2 and w not in words:\n",
    "                words.add(w)\n",
    "                if len(words) >= 20:\n",
    "                    break\n",
    "    \n",
    "    # Return top 15 most relevant suggestions\n",
    "    return sorted(list(words))[:15]\n",
    "\n",
    "print(\"Image-based recommendation function ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Build Vocabulary and Create CNN-RNN Model from Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.10. CNN-RNN Training Function\n",
    "\n",
    "# Import required modules (if not already imported)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_cnn_rnn_model(model, dataloader, vocab, device, num_epochs=10, learning_rate=4e-4, \n",
    "                        grad_clip=5.0, print_freq=50, grad_accum_steps=1):\n",
    "    \"\"\"\n",
    "    Train CNN-RNN model with proper loss calculation and attention regularization\n",
    "    \n",
    "    Args:\n",
    "        model: ImageCaptionModel instance\n",
    "        dataloader: DataLoader with CNNRNNDataset\n",
    "        vocab: Vocabulary object\n",
    "        device: torch device\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        grad_clip: Gradient clipping value\n",
    "        print_freq: Frequency of printing loss\n",
    "        grad_accum_steps: Gradient accumulation steps\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    # Use mixed precision training if available\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        optimizer.zero_grad()\n",
    "        total_batches = len(dataloader)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            caption_lengths = batch['caption_lengths'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                predictions, encoded_captions_sorted, decode_lengths, alphas, sort_ind = model(\n",
    "                    images, captions, caption_lengths\n",
    "                )\n",
    "                \n",
    "                # Prepare targets (shift by 1 to align with predictions)\n",
    "                targets = encoded_captions_sorted[:, 1:1+predictions.size(1)]\n",
    "                \n",
    "                # Create mask for valid positions (excluding padding)\n",
    "                if isinstance(decode_lengths, list):\n",
    "                    decode_lengths_tensor = torch.tensor(decode_lengths, device=device, dtype=torch.long)\n",
    "                else:\n",
    "                    decode_lengths_tensor = decode_lengths\n",
    "                \n",
    "                batch_size = predictions.size(0)\n",
    "                max_decode_len = predictions.size(1)\n",
    "                lengths_expanded = decode_lengths_tensor.unsqueeze(1).expand(batch_size, max_decode_len)\n",
    "                positions = torch.arange(max_decode_len, device=device).unsqueeze(0).expand(batch_size, max_decode_len)\n",
    "                mask = positions < lengths_expanded\n",
    "                \n",
    "                # Flatten and apply mask\n",
    "                predictions_flat = predictions.reshape(-1, predictions.size(-1))\n",
    "                targets_flat = targets.reshape(-1)\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                \n",
    "                # Calculate loss only on valid positions\n",
    "                if mask_flat.any():\n",
    "                    valid_preds = predictions_flat[mask_flat]\n",
    "                    valid_targets = targets_flat[mask_flat]\n",
    "                    loss = criterion(valid_preds, valid_targets)\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "                \n",
    "                # Attention regularization to encourage proper attention\n",
    "                attention_reg = 0.5 * ((1.0 - alphas.sum(dim=1)) ** 2).mean()\n",
    "                loss = loss + attention_reg\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            epoch_loss += loss_value\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = loss / grad_accum_steps\n",
    "            scaler.scale(scaled_loss).backward()\n",
    "            \n",
    "            # Update weights\n",
    "            should_step = ((batch_idx + 1) % grad_accum_steps == 0) or ((batch_idx + 1) == total_batches)\n",
    "            if should_step:\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if (batch_idx + 1) % print_freq == 0:\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': loss_value,\n",
    "                    'avg_loss': epoch_loss / num_batches\n",
    "                })\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f} - LR: {current_lr:.2e}\")\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "print(\"CNN-RNN training function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.11. Train CNN-RNN Model\n",
    "\n",
    "# IMPORTANT: Make sure you have run the following cells in order:\n",
    "# 1. Dataset loading cell (loads 'ds')\n",
    "# 2. Model creation cell (creates 'vocab' and 'cnn_rnn_model') - Cell with \"## 1.9. Build Vocabulary...\"\n",
    "# 3. Training function cell (defines 'train_cnn_rnn_model') - Cell with \"## 1.10. CNN-RNN Training Function\"\n",
    "# 4. This cell (executes training)\n",
    "\n",
    "# Import required modules\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if training function is defined\n",
    "if 'train_cnn_rnn_model' not in globals():\n",
    "    print(\"ERROR: Training function 'train_cnn_rnn_model' not found!\")\n",
    "    print(\"Please run the cell with '## 1.10. CNN-RNN Training Function' first.\")\n",
    "elif 'ds' not in locals():\n",
    "    print(\"ERROR: Dataset 'ds' not found!\")\n",
    "    print(\"Please run the dataset loading cell first.\")\n",
    "elif 'cnn_rnn_model' not in locals() or 'vocab' not in locals():\n",
    "    print(\"ERROR: Model or vocabulary not found!\")\n",
    "    print(\"Please run the cell with '## 1.9. Build Vocabulary and Create CNN-RNN Model from Dataset' first.\")\n",
    "elif 'ds' in locals() and 'cnn_rnn_model' in locals() and 'vocab' in locals():\n",
    "    print(\"Setting up CNN-RNN training...\")\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_dataset = CNNRNNDataset(\n",
    "        hf_dataset=ds,\n",
    "        vocab=vocab,\n",
    "        max_length=50,\n",
    "        split='train',\n",
    "        use_yolo=False  # Set to True if you want YOLO enhancement during training\n",
    "    )\n",
    "    \n",
    "    # Determine batch size based on dataset size\n",
    "    dataset_size = len(train_dataset)\n",
    "    if dataset_size < 50:\n",
    "        batch_size = 2\n",
    "        grad_accum_steps = 4\n",
    "    elif dataset_size < 200:\n",
    "        batch_size = 4\n",
    "        grad_accum_steps = 2\n",
    "    elif dataset_size < 500:\n",
    "        batch_size = 8\n",
    "        grad_accum_steps = 2\n",
    "    else:\n",
    "        batch_size = 16\n",
    "        grad_accum_steps = 1\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=cnn_rnn_collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}, Gradient accumulation steps: {grad_accum_steps}\")\n",
    "    print(f\"Number of batches per epoch: {len(train_dataloader)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting CNN-RNN Model Training\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    num_epochs = 15  # Increase epochs for better training\n",
    "    train_losses = train_cnn_rnn_model(\n",
    "        model=cnn_rnn_model,\n",
    "        dataloader=train_dataloader,\n",
    "        vocab=vocab,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=4e-4,\n",
    "        grad_clip=5.0,\n",
    "        print_freq=10,\n",
    "        grad_accum_steps=grad_accum_steps\n",
    "    )\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('CNN-RNN Training Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "    print(\"\\nThe model is now ready to generate captions.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot train: Dataset, model, or vocabulary not found.\")\n",
    "    print(\"Please run the dataset loading and model creation cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from dataset and create CNN-RNN model\n",
    "# This should be run after loading the dataset (ds)\n",
    "# Ensure device is set\n",
    "if 'device' not in locals():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device set to: {device}\")\n",
    "\n",
    "if 'ds' in locals() and len(ds['train']) > 0:\n",
    "    print(\"Building vocabulary from dataset...\")\n",
    "    \n",
    "    # Collect all captions from the dataset\n",
    "    all_captions = []\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in ds:\n",
    "            for example in ds[split]:\n",
    "                # Flickr8k has caption_0, caption_1, etc.\n",
    "                for i in range(5):  # Usually 5 captions per image\n",
    "                    caption_key = f'caption_{i}'\n",
    "                    if caption_key in example:\n",
    "                        all_captions.append(example[caption_key])\n",
    "                    elif i == 0 and 'caption' in example:\n",
    "                        all_captions.append(example['caption'])\n",
    "                        break\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = Vocabulary(freq_threshold=1)  # Lower threshold for more words\n",
    "    vocab.build_vocabulary(all_captions)\n",
    "    print(f\"Vocabulary built with {len(vocab)} words from {len(all_captions)} captions\")\n",
    "    \n",
    "    # Create CNN-RNN model\n",
    "    vocab_size = len(vocab)\n",
    "    cnn_rnn_model = ImageCaptionModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=256,\n",
    "        decoder_dim=512,\n",
    "        attention_dim=256,\n",
    "        encoder_name='resnet50',\n",
    "        dropout=0.5,\n",
    "        fine_tune_encoder=True,\n",
    "        decoder_type='lstm'\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"CNN-RNN model created with vocab size: {vocab_size}\")\n",
    "    print(\"Note: Model needs to be trained before generating captions. Use the training cell below.\")\n",
    "else:\n",
    "    print(\"Dataset not loaded yet. Please run the dataset loading cell first.\")\n",
    "    print(\"CNN-RNN model will be created after dataset is loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import ipywidgets, make it optional\n",
    "try:\n",
    "    from ipywidgets import FileUpload, Button, Output\n",
    "    IPYWIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: ipywidgets not available. Image upload widget will not work.\")\n",
    "    print(\"Install with: pip install ipywidgets jupyter\")\n",
    "    IPYWIDGETS_AVAILABLE = False\n",
    "    FileUpload = None\n",
    "    Button = None\n",
    "    Output = None\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Import transformers after numpy is set\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error importing transformers: {e}\")\n",
    "    print(\"Trying to fix numpy compatibility...\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Collate Function and Dataset Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle mixed tensor and string data\"\"\"\n",
    "    # Stack pixel values (images are already the same size)\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    input_ids_list = [item['input_ids'] for item in batch]\n",
    "    attention_mask_list = [item['attention_mask'] for item in batch]\n",
    "    \n",
    "    # Find max length in the batch\n",
    "    max_length = max(len(ids) for ids in input_ids_list)\n",
    "    \n",
    "    # Get pad token id from processor (access from global scope)\n",
    "    try:\n",
    "        # Try to get pad_token_id from processor if it exists\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "        if pad_token_id is None:\n",
    "            pad_token_id = 0\n",
    "    except (NameError, AttributeError):\n",
    "        # Fallback to 0 if processor not available yet\n",
    "        pad_token_id = 0\n",
    "    \n",
    "    # Pad input_ids and attention_mask\n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    \n",
    "    for ids, mask in zip(input_ids_list, attention_mask_list):\n",
    "        pad_length = max_length - len(ids)\n",
    "        if pad_length > 0:\n",
    "            # Pad input_ids with pad_token_id\n",
    "            padded_ids = torch.cat([ids, torch.full((pad_length,), pad_token_id, dtype=ids.dtype)])\n",
    "            # Pad attention_mask with 0\n",
    "            padded_mask = torch.cat([mask, torch.zeros(pad_length, dtype=mask.dtype)])\n",
    "        else:\n",
    "            padded_ids = ids\n",
    "            padded_mask = mask\n",
    "        \n",
    "        padded_input_ids.append(padded_ids)\n",
    "        padded_attention_mask.append(padded_mask)\n",
    "    \n",
    "    # Stack padded tensors\n",
    "    input_ids = torch.stack(padded_input_ids)\n",
    "    attention_mask = torch.stack(padded_attention_mask)\n",
    "    \n",
    "    # Keep strings as lists (not tensors)\n",
    "    captions = [item.get('caption', '') for item in batch]\n",
    "    image_ids = [item.get('image_id', '') for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'caption': captions,\n",
    "        'image_id': image_ids\n",
    "    }\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    \"\"\"Dataset for Image Captioning using Hugging Face Flickr8k dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, hf_dataset, processor, max_length=50, transform=None, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset: Hugging Face dataset (DatasetDict or Dataset)\n",
    "            processor: BLIP processor\n",
    "            max_length: Maximum caption length\n",
    "            transform: Optional image transforms\n",
    "            split: Dataset split to use ('train', 'validation', 'test')\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract the split from DatasetDict or use the dataset directly\n",
    "        if hasattr(hf_dataset, 'keys'):\n",
    "            # It's a DatasetDict\n",
    "            if split in hf_dataset:\n",
    "                self.dataset = hf_dataset[split]\n",
    "            else:\n",
    "                raise ValueError(f\"Split '{split}' not found in dataset. Available splits: {list(hf_dataset.keys())}\")\n",
    "        else:\n",
    "            # It's already a Dataset\n",
    "            self.dataset = hf_dataset\n",
    "        \n",
    "        print(f\"Loaded {len(self.dataset)} examples from {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Get image (already a PIL Image from Hugging Face)\n",
    "        image = example['image']\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        # Get caption - Flickr8k has caption_0, caption_1, etc.\n",
    "        # Use caption_0 as the primary caption, but you can randomize if needed\n",
    "        caption = example.get('caption_0', example.get('caption', 'a photo'))\n",
    "        \n",
    "        # Apply transform if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Process image and text\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        # Get image ID if available\n",
    "        image_id = example.get('image_id', example.get('id', f'img_{idx}'))\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'caption': caption,\n",
    "            'image_id': str(image_id)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Setup - Using BLIP (Bootstrapping Language-Image Pre-training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading BLIP model (this may take a few minutes on first run)...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BLIP model and processor\n",
    "if not TRANSFORMERS_AVAILABLE:\n",
    "    print(\"ERROR: Transformers library not available. Please restart kernel and run the installation cell again.\")\n",
    "    print(\"If the issue persists, try: %pip install --upgrade transformers\")\n",
    "    raise ImportError(\"Transformers library required for this notebook\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained BLIP model\n",
    "try:\n",
    "    model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "    print(\"Loading BLIP model (this may take a few minutes on first run)...\")\n",
    "    processor = BlipProcessor.from_pretrained(model_name)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure you have internet connection for first-time download.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Function with Epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, device, num_epochs=3):\n",
    "    \"\"\"Train the image captioning model\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics (BLEU Score for Accuracy/Precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"Calculate BLEU score between reference and candidate captions\"\"\"\n",
    "    try:\n",
    "        ref_tokens = word_tokenize(reference.lower())\n",
    "        cand_tokens = word_tokenize(candidate.lower())\n",
    "        \n",
    "        smoothing = SmoothingFunction().method1\n",
    "        score = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=smoothing)\n",
    "        return score\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_model(model, dataloader, processor, device, num_samples=50):\n",
    "    \"\"\"Evaluate model and calculate BLEU scores\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_bleu_scores = []\n",
    "    all_precisions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            if idx >= num_samples:\n",
    "                break\n",
    "            \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            reference_caption = batch['caption'][0]  # Get first caption from batch\n",
    "            \n",
    "            # Generate caption - BLIP generate accepts pixel_values directly\n",
    "            try:\n",
    "                generated_ids = model.generate(\n",
    "                    pixel_values=pixel_values,\n",
    "                    max_length=50,\n",
    "                    num_beams=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                \n",
    "                # Decode the first generated caption (handle batch dimension)\n",
    "                if generated_ids.dim() > 1:\n",
    "                    generated_caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                else:\n",
    "                    generated_caption = processor.decode(generated_ids, skip_special_tokens=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating caption for sample {idx}: {e}\")\n",
    "                generated_caption = \"\"\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu = calculate_bleu_score(reference_caption, generated_caption)\n",
    "            all_bleu_scores.append(bleu)\n",
    "            \n",
    "            # Calculate precision (word overlap)\n",
    "            ref_words = set(word_tokenize(reference_caption.lower()))\n",
    "            gen_words = set(word_tokenize(generated_caption.lower()))\n",
    "            if len(gen_words) > 0:\n",
    "                precision = len(ref_words.intersection(gen_words)) / len(gen_words)\n",
    "            else:\n",
    "                precision = 0.0\n",
    "            all_precisions.append(precision)\n",
    "    \n",
    "    avg_bleu = np.mean(all_bleu_scores) if all_bleu_scores else 0.0\n",
    "    avg_precision = np.mean(all_precisions) if all_precisions else 0.0\n",
    "    \n",
    "    return avg_bleu, avg_precision, all_bleu_scores, all_precisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Sample Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample images...\n",
      "Downloaded and processed image 1\n",
      "Downloaded and processed image 2\n",
      "Downloaded and processed image 3\n",
      "Downloaded and processed image 4\n",
      "Downloaded and processed image 5\n",
      "Dataset prepared!\n"
     ]
    }
   ],
   "source": [
    "# Load Flickr8k dataset from Hugging Face\n",
    "print(\"Loading Flickr8k dataset from Hugging Face...\")\n",
    "print(\"This may take a few minutes on first run as it downloads the dataset...\")\n",
    "\n",
    "try:\n",
    "    ds = load_dataset(\"jxie/flickr8k\")\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Available splits: {list(ds.keys())}\")\n",
    "    print(f\"Train split size: {len(ds['train'])}\")\n",
    "    if 'validation' in ds:\n",
    "        print(f\"Validation split size: {len(ds['validation'])}\")\n",
    "    if 'test' in ds:\n",
    "        print(f\"Test split size: {len(ds['test'])}\")\n",
    "    \n",
    "    # Display a few examples\n",
    "    print(\"\\nDisplaying sample images from the dataset...\")\n",
    "    n_display = 5\n",
    "    examples = ds[\"train\"].select(list(range(min(n_display, len(ds[\"train\"])))))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, ex in enumerate(examples):\n",
    "        img = ex[\"image\"]  # PIL Image object\n",
    "        # Get the first caption (caption_0)\n",
    "        caption = ex.get(\"caption_0\", ex.get(\"caption\", \"No caption\"))\n",
    "        \n",
    "        plt.subplot(1, n_display, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(caption[:50] + \"...\" if len(caption) > 50 else caption, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDataset prepared and ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Make sure you have internet connection for first-time download.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We use the custom collate_fn defined earlier, not DataCollatorWithPadding\n",
    "# The custom collate_fn properly handles string fields (caption, image_id) along with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|                                                                                 | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`_image_id` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:767\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 767\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    771\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:729\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 23\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:272\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 272\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    281\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:67\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3374\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3371\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3372\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:783\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    779\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    784\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    788\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`_image_id` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader using Flickr8k from Hugging Face\n",
    "# Make sure 'ds' is defined from the previous cell\n",
    "if 'ds' not in locals():\n",
    "    print(\"Error: Dataset 'ds' not found. Please run the previous cell to load the dataset.\")\n",
    "else:\n",
    "    # Create training dataset\n",
    "    train_dataset = Flickr8kDataset(\n",
    "        hf_dataset=ds,\n",
    "        processor=processor,\n",
    "        max_length=50,\n",
    "        split='train'\n",
    "    )\n",
    "    \n",
    "    if len(train_dataset) > 0:\n",
    "        # Create dataloader with appropriate batch size\n",
    "        # Use smaller batch size if you have memory constraints\n",
    "        batch_size = 4  # Adjust based on your GPU/CPU memory\n",
    "        dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0  # Set to 0 to avoid multiprocessing issues in notebooks\n",
    "        )\n",
    "        \n",
    "        print(f\"Created dataloader with batch size {batch_size}\")\n",
    "        print(f\"Number of batches per epoch: {len(dataloader)}\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\nStarting training...\")\n",
    "        num_epochs = 3\n",
    "        train_losses = train_model(model, dataloader, optimizer, device, num_epochs=num_epochs)\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Epochs')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "    else:\n",
    "        print(\"Dataset is empty. Using pre-trained model for inference only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation - Calculate Accuracy and Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# Use validation split if available, otherwise use a subset of training data\n",
    "if 'ds' in locals():\n",
    "    # Try to use validation split, otherwise use test split, otherwise use train\n",
    "    if 'validation' in ds:\n",
    "        eval_split = 'validation'\n",
    "    elif 'test' in ds:\n",
    "        eval_split = 'test'\n",
    "    else:\n",
    "        eval_split = 'train'\n",
    "    \n",
    "    eval_dataset = Flickr8kDataset(\n",
    "        hf_dataset=ds,\n",
    "        processor=processor,\n",
    "        max_length=50,\n",
    "        split=eval_split\n",
    "    )\n",
    "    \n",
    "    if len(eval_dataset) > 0:\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset, \n",
    "            batch_size=1, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        print(f\"Evaluating model on {eval_split} split ({len(eval_dataset)} examples)...\")\n",
    "        num_eval_samples = min(50, len(eval_dataset))  # Evaluate on up to 50 samples\n",
    "        avg_bleu, avg_precision, bleu_scores, precisions = evaluate_model(\n",
    "            model, eval_dataloader, processor, device, num_samples=num_eval_samples\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"EVALUATION RESULTS ({eval_split.upper()} SPLIT)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Average BLEU Score (Accuracy): {avg_bleu:.4f}\")\n",
    "        print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"BLEU Score Range: {min(bleu_scores):.4f} - {max(bleu_scores):.4f}\")\n",
    "        print(f\"Precision Range: {min(precisions):.4f} - {max(precisions):.4f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Plot metrics\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        ax1.bar(range(len(bleu_scores)), bleu_scores)\n",
    "        ax1.set_xlabel('Sample')\n",
    "        ax1.set_ylabel('BLEU Score')\n",
    "        ax1.set_title('BLEU Scores per Sample')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.bar(range(len(precisions)), precisions, color='orange')\n",
    "        ax2.set_xlabel('Sample')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_title('Precision per Sample')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No dataset available for evaluation. Model is ready for inference.\")\n",
    "else:\n",
    "    print(\"No dataset available for evaluation. Please load the dataset first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Image Upload and Caption Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_both_captions(image_path_or_url, transformer_model, processor, cnn_rnn_model, vocab, device, use_yolo=False, beam_size=None):\n",
    "    \"\"\"Generate both Transformer and CNN-RNN captions for the same image.\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            image_pil = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            image_pil = Image.open(image_path_or_url).convert('RGB')\n",
    "        \n",
    "        # Generate Transformer caption\n",
    "        transformer_caption = None\n",
    "        if transformer_model is not None and processor is not None:\n",
    "            transformer_caption = generate_transformer_caption_from_pil(image_pil, transformer_model, processor, device)\n",
    "        \n",
    "        # Generate CNN-RNN caption\n",
    "        cnn_rnn_caption = None\n",
    "        if cnn_rnn_model is not None and vocab is not None:\n",
    "            _, cnn_rnn_caption = generate_cnn_rnn_caption(\n",
    "                image_path_or_url, cnn_rnn_model, vocab, device, use_yolo=use_yolo, beam_size=beam_size\n",
    "            )\n",
    "        \n",
    "        return image_pil, transformer_caption, cnn_rnn_caption\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return None, None, f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n",
    "\n",
    "# Interactive image upload widget with dual caption generation and word suggestions\n",
    "if IPYWIDGETS_AVAILABLE:\n",
    "    from ipywidgets import Textarea, VBox, Label, HTML as HTMLWidget\n",
    "    \n",
    "    upload_widget = FileUpload(\n",
    "        accept='image/*',\n",
    "        multiple=False,\n",
    "        description='Upload Image'\n",
    "    )\n",
    "    \n",
    "    # Create comment box and suggestion label (will be shown after image upload)\n",
    "    comment_box = Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"Write your own caption based on the image...\",\n",
    "        description=\"Your caption:\",\n",
    "        layout={\"width\": \"100%\", \"height\": \"100px\"},\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    suggestion_label = Label(\n",
    "        value=\"Suggestions will appear here after you start typing...\",\n",
    "        layout={\"width\": \"100%\"}\n",
    "    )\n",
    "    \n",
    "    # Container for captions and comment box\n",
    "    caption_output = Output()\n",
    "    comment_output = Output()  # Separate output for comment box\n",
    "    comment_container = VBox([comment_box, suggestion_label])\n",
    "    \n",
    "    def on_comment_change(change):\n",
    "        \"\"\"Update recommendations as user types\"\"\"\n",
    "        text = change['new']\n",
    "        global _last_uploaded_image, _last_cnn_caption, _last_transformer_caption, _last_yolo_objects\n",
    "        suggestions = get_image_based_recommendations(\n",
    "            text,\n",
    "            image_pil=_last_uploaded_image,\n",
    "            cnn_caption=_last_cnn_caption,\n",
    "            transformer_caption=_last_transformer_caption,\n",
    "            yolo_objects=_last_yolo_objects,\n",
    "            vocab=vocab if 'vocab' in globals() else None\n",
    "        )\n",
    "        if suggestions:\n",
    "            suggestion_label.value = \"ðŸ’¡ Suggestions: \" + \", \".join(suggestions)\n",
    "        else:\n",
    "            suggestion_label.value = \"ðŸ’¡ Suggestions: (type to see recommendations)\"\n",
    "    \n",
    "    comment_box.observe(on_comment_change, names='value')\n",
    "    \n",
    "    def on_upload_change(change):\n",
    "        \"\"\"Handle image upload and generate both captions\"\"\"\n",
    "        global _last_uploaded_image, _last_cnn_caption, _last_transformer_caption, _last_yolo_objects\n",
    "        \n",
    "        with caption_output:\n",
    "            caption_output.clear_output()\n",
    "            \n",
    "            if len(upload_widget.value) > 0:\n",
    "                try:\n",
    "                    # Handle different ipywidgets versions\n",
    "                    if isinstance(upload_widget.value, dict):\n",
    "                        uploaded_file = list(upload_widget.value.values())[0]\n",
    "                    elif isinstance(upload_widget.value, (list, tuple)):\n",
    "                        uploaded_file = upload_widget.value[0]\n",
    "                    else:\n",
    "                        uploaded_file = upload_widget.value\n",
    "                    \n",
    "                    # Extract content\n",
    "                    if isinstance(uploaded_file, dict):\n",
    "                        file_content = uploaded_file.get('content', uploaded_file.get('data', None))\n",
    "                    else:\n",
    "                        file_content = uploaded_file\n",
    "                    \n",
    "                    if file_content is None:\n",
    "                        print(\"Error: Could not extract file content\")\n",
    "                        return\n",
    "                    \n",
    "                    # Save uploaded image temporarily\n",
    "                    with open('temp_uploaded_image.jpg', 'wb') as f:\n",
    "                        if isinstance(file_content, bytes):\n",
    "                            f.write(file_content)\n",
    "                        else:\n",
    "                            f.write(file_content.encode() if isinstance(file_content, str) else bytes(file_content))\n",
    "                    \n",
    "                    print(\"ðŸ”„ Processing image and generating captions...\")\n",
    "                    \n",
    "                    # Check if models are defined\n",
    "                    transformer_model_available = 'model' in globals() and model is not None and 'processor' in globals() and processor is not None\n",
    "                    cnn_rnn_model_available = 'cnn_rnn_model' in globals() and cnn_rnn_model is not None and 'vocab' in globals() and vocab is not None\n",
    "                    \n",
    "                    if not transformer_model_available and not cnn_rnn_model_available:\n",
    "                        print(\"Error: No models available. Please run the model loading/training cells first.\")\n",
    "                        return\n",
    "                    \n",
    "                    # Load image for YOLO detection\n",
    "                    image_pil = Image.open('temp_uploaded_image.jpg').convert('RGB')\n",
    "                    _last_uploaded_image = image_pil\n",
    "                    \n",
    "                    # Detect objects with YOLO\n",
    "                    yolo_objects = []\n",
    "                    if 'yolo_detector' in globals() and yolo_detector.model is not None:\n",
    "                        try:\n",
    "                            yolo_objects = yolo_detector.detect_objects(image_pil)\n",
    "                            _last_yolo_objects = yolo_objects\n",
    "                        except:\n",
    "                            _last_yolo_objects = []\n",
    "                    else:\n",
    "                        _last_yolo_objects = []\n",
    "                    \n",
    "                    # Generate both captions\n",
    "                    image, transformer_caption, cnn_rnn_caption = generate_both_captions(\n",
    "                        'temp_uploaded_image.jpg',\n",
    "                        model if transformer_model_available else None,\n",
    "                        processor if transformer_model_available else None,\n",
    "                        cnn_rnn_model if cnn_rnn_model_available else None,\n",
    "                        vocab if cnn_rnn_model_available else None,\n",
    "                        device,\n",
    "                        use_yolo=True,\n",
    "                        beam_size=None\n",
    "                    )\n",
    "                    \n",
    "                    if image:\n",
    "                        # Store for recommendations\n",
    "                        _last_transformer_caption = transformer_caption\n",
    "                        _last_cnn_caption = cnn_rnn_caption\n",
    "                        \n",
    "                        # Display image and both captions\n",
    "                        plt.figure(figsize=(12, 8))\n",
    "                        plt.imshow(image)\n",
    "                        plt.axis('off')\n",
    "                        caption_lines = []\n",
    "                        if transformer_caption:\n",
    "                            caption_lines.append(f\"Transformer: {transformer_caption}\")\n",
    "                        if cnn_rnn_caption:\n",
    "                            caption_lines.append(f\"CNN-RNN: {cnn_rnn_caption}\")\n",
    "                        if not caption_lines:\n",
    "                            caption_lines.append(\"No captions generated\")\n",
    "                        plt.title(\"\\n\\n\".join(caption_lines), fontsize=14, pad=20)\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        print(f\"\\n{'='*70}\")\n",
    "                        if transformer_caption:\n",
    "                            print(f\"ðŸ“ TRANSFORMER CAPTION: {transformer_caption}\")\n",
    "                        if cnn_rnn_caption:\n",
    "                            print(f\"ðŸ“ CNN-RNN CAPTION   : {cnn_rnn_caption}\")\n",
    "                        if yolo_objects:\n",
    "                            print(f\"ðŸ” Detected objects: {', '.join(yolo_objects[:5])}\")\n",
    "                        print(f\"{'='*70}\\n\")\n",
    "                        \n",
    "                        # Show comment box after captions are generated\n",
    "                        comment_box.value = \"\"  # Clear previous text\n",
    "                        with comment_output:\n",
    "                            comment_output.clear_output()\n",
    "                            display(HTMLWidget(\"<h4>âœï¸ Write your own caption (with smart suggestions):</h4>\"))\n",
    "                            display(comment_container)\n",
    "                    else:\n",
    "                        print(f\"Error: {cnn_rnn_caption or transformer_caption}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing uploaded file: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "    \n",
    "    upload_widget.observe(on_upload_change, names='value')\n",
    "    \n",
    "    display(HTML(\"<h3>ðŸ“¤ Upload an image to generate captions:</h3>\"))\n",
    "    display(upload_widget)\n",
    "    display(caption_output)\n",
    "    display(comment_output)  # Display comment box area (will be populated after upload)\n",
    "else:\n",
    "    display(HTML(\"<h3>Image Upload Widget</h3>\"))\n",
    "    display(HTML(\"<p>ipywidgets is not available. Please use the <code>caption_image()</code> function instead.</p>\"))\n",
    "    display(HTML(\"<p>To enable the upload widget, install: <code>pip install ipywidgets jupyter</code> and restart the kernel.</p>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Humanization for Captions\n",
    "# This cell creates functions to convert generated captions into more human-written style\n",
    "# Uses rule-based text transformations to preserve meaning while improving readability\n",
    "\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    GPT2_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: GPT-2 not available. Install with: pip install transformers\")\n",
    "    GPT2_AVAILABLE = False\n",
    "    GPT2LMHeadModel = None\n",
    "    GPT2Tokenizer = None\n",
    "\n",
    "# Initialize GPT-2 model for text generation\n",
    "_humanize_model = None\n",
    "_humanize_tokenizer = None\n",
    "\n",
    "def load_humanize_model():\n",
    "    \"\"\"Load GPT-2 model for humanizing captions\"\"\"\n",
    "    global _humanize_model, _humanize_tokenizer\n",
    "    if not GPT2_AVAILABLE:\n",
    "        return False\n",
    "    \n",
    "    if _humanize_model is None:\n",
    "        try:\n",
    "            print(\"Loading GPT-2 model for humanizing captions...\")\n",
    "            model_name = \"gpt2\"  # Using GPT-2 base model\n",
    "            _humanize_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "            _humanize_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "            _humanize_tokenizer.pad_token = _humanize_tokenizer.eos_token\n",
    "            \n",
    "            # Move to device if available\n",
    "            if 'device' in globals():\n",
    "                _humanize_model = _humanize_model.to(device)\n",
    "            else:\n",
    "                _humanize_model = _humanize_model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            \n",
    "            _humanize_model.eval()\n",
    "            print(\"GPT-2 model loaded successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading GPT-2 model: {e}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def humanize_caption(caption, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Convert a generated caption into a more human-written style using text transformations\n",
    "    This preserves the meaning while improving readability and naturalness\n",
    "    \n",
    "    Args:\n",
    "        caption: The original caption to humanize\n",
    "        max_length: Not used (kept for compatibility)\n",
    "        temperature: Not used (kept for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        Humanized caption string\n",
    "    \"\"\"\n",
    "    if not caption or len(caption.strip()) < 5:\n",
    "        return caption  # Skip very short captions\n",
    "    \n",
    "    try:\n",
    "        # Remove YOLO detection artifacts\n",
    "        if \"(contains:\" in caption:\n",
    "            caption = caption.split(\"(contains:\")[0].strip()\n",
    "        \n",
    "        # Start with the original caption\n",
    "        humanized = caption.strip()\n",
    "        \n",
    "        # 1. Fix common patterns to make it more natural\n",
    "        replacements = {\n",
    "            # Make descriptions more natural\n",
    "            \"a man in a\": \"a man wearing\",\n",
    "            \"a woman in a\": \"a woman wearing\",\n",
    "            \"a person in a\": \"a person wearing\",\n",
    "            \"is sitting at\": \"sits at\",\n",
    "            \"is standing\": \"stands\",\n",
    "            \"is looking at\": \"looks at\",\n",
    "            \"is holding\": \"holds\",\n",
    "            \"is wearing\": \"wears\",\n",
    "            # Remove redundant \"in front of\" when followed by objects\n",
    "            \"in front of a\": \"near a\",\n",
    "            \"in front of the\": \"near the\",\n",
    "            # Make it more conversational\n",
    "            \"there is\": \"there's\",\n",
    "            \"there are\": \"there're\",\n",
    "            # Fix \"a photo\" or \"an image\" at the start\n",
    "            \"^a photo of \": \"\",\n",
    "            \"^an image of \": \"\",\n",
    "            \"^a picture of \": \"\",\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            if old.startswith(\"^\"):\n",
    "                # Pattern at start\n",
    "                pattern = old[1:]\n",
    "                if humanized.lower().startswith(pattern):\n",
    "                    humanized = new + humanized[len(pattern):]\n",
    "            else:\n",
    "                humanized = humanized.replace(old, new)\n",
    "        \n",
    "        # 2. Capitalize first letter\n",
    "        if humanized:\n",
    "            humanized = humanized[0].upper() + humanized[1:] if len(humanized) > 1 else humanized.upper()\n",
    "        \n",
    "        # 3. Ensure proper punctuation\n",
    "        if humanized and not humanized.endswith(('.', '!', '?')):\n",
    "            humanized = humanized.rstrip(',;:') + '.'\n",
    "        \n",
    "        # 4. Remove extra spaces\n",
    "        import re\n",
    "        humanized = re.sub(r'\\s+', ' ', humanized).strip()\n",
    "        \n",
    "        # 5. Fix double articles (e.g., \"a a man\" -> \"a man\")\n",
    "        humanized = re.sub(r'\\ba\\s+a\\s+', 'a ', humanized)\n",
    "        humanized = re.sub(r'\\ban\\s+an\\s+', 'an ', humanized)\n",
    "        humanized = re.sub(r'\\bthe\\s+the\\s+', 'the ', humanized)\n",
    "        \n",
    "        # 6. Make it more natural by using contractions where appropriate\n",
    "        # (but keep it simple - don't overdo it)\n",
    "        \n",
    "        # 7. If the caption is too technical/robotic, add a bit of variety\n",
    "        # Check if it starts with very common patterns\n",
    "        if humanized.lower().startswith((\"a man\", \"a woman\", \"a person\", \"a group\")):\n",
    "            # Try to make it slightly more varied while keeping meaning\n",
    "            # But don't change the core content\n",
    "            pass  # Keep as is for now\n",
    "        \n",
    "        # 8. Remove any remaining artifacts\n",
    "        humanized = humanized.replace(\"  \", \" \").strip()\n",
    "        \n",
    "        # Validate: make sure we didn't break it\n",
    "        if len(humanized) < 3:\n",
    "            return caption\n",
    "        \n",
    "        # If the result is too different from original, return original\n",
    "        # (we want to improve style, not change meaning)\n",
    "        original_words = set(caption.lower().split())\n",
    "        humanized_words = set(humanized.lower().split())\n",
    "        \n",
    "        # Keep at least 70% of original words\n",
    "        if len(original_words) > 0:\n",
    "            overlap = len(original_words.intersection(humanized_words)) / len(original_words)\n",
    "            if overlap < 0.7:\n",
    "                return caption  # Too different, return original\n",
    "        \n",
    "        return humanized\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error humanizing caption: {e}\")\n",
    "        return caption  # Return original on error\n",
    "\n",
    "def humanize_both_captions(transformer_caption, cnn_rnn_caption):\n",
    "    \"\"\"\n",
    "    Humanize both transformer and CNN-RNN captions\n",
    "    \n",
    "    Args:\n",
    "        transformer_caption: Caption from transformer model\n",
    "        cnn_rnn_caption: Caption from CNN-RNN model\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (humanized_transformer, humanized_cnn_rnn)\n",
    "    \"\"\"\n",
    "    humanized_transformer = transformer_caption\n",
    "    humanized_cnn_rnn = cnn_rnn_caption\n",
    "    \n",
    "    if transformer_caption:\n",
    "        humanized_transformer = humanize_caption(transformer_caption)\n",
    "    \n",
    "    if cnn_rnn_caption:\n",
    "        humanized_cnn_rnn = humanize_caption(cnn_rnn_caption)\n",
    "    \n",
    "    return humanized_transformer, humanized_cnn_rnn\n",
    "\n",
    "# Wrap the generate_both_captions function to automatically humanize captions\n",
    "# This will intercept calls and humanize the captions before returning them\n",
    "\n",
    "# Use a module-level variable to store the original function (prevents recursion)\n",
    "if '_gpt2_original_generate_both_captions' not in globals():\n",
    "    # First time - store the original function\n",
    "    if 'generate_both_captions' in globals():\n",
    "        # Check if it's already our wrapper\n",
    "        current_func = globals()['generate_both_captions']\n",
    "        if hasattr(current_func, '__name__') and current_func.__name__ == 'generate_both_captions_with_humanization':\n",
    "            # Already wrapped, get the original from the stored reference\n",
    "            if '_gpt2_original_generate_both_captions' in globals():\n",
    "                print(\"Function already wrapped, using stored original...\")\n",
    "            else:\n",
    "                print(\"Warning: Function appears wrapped but original not found. Please restart kernel and run cells in order.\")\n",
    "                _gpt2_original_generate_both_captions = None\n",
    "        else:\n",
    "            # Not wrapped yet, store the original\n",
    "            globals()['_gpt2_original_generate_both_captions'] = current_func\n",
    "            print(f\"Found generate_both_captions function, storing original...\")\n",
    "    else:\n",
    "        globals()['_gpt2_original_generate_both_captions'] = None\n",
    "        print(\"Warning: generate_both_captions not found yet. Run the upload widget cell first, then re-run this cell.\")\n",
    "else:\n",
    "    print(\"Original function already stored, reusing it...\")\n",
    "\n",
    "def generate_both_captions_with_humanization(image_path_or_url, transformer_model, processor, cnn_rnn_model, vocab, device, use_yolo=False, beam_size=None):\n",
    "    \"\"\"Wrapper that generates captions and then humanizes them\"\"\"\n",
    "    # Get the original function from globals\n",
    "    original_func = globals().get('_gpt2_original_generate_both_captions', None)\n",
    "    \n",
    "    if original_func is None:\n",
    "        print(\"Error: Original generate_both_captions not available\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Generate original captions using the stored original function\n",
    "    image, transformer_caption, cnn_rnn_caption = original_func(\n",
    "        image_path_or_url, transformer_model, processor, cnn_rnn_model, vocab, device, use_yolo, beam_size\n",
    "    )\n",
    "    \n",
    "    # Humanize the captions if they exist\n",
    "    if image and (transformer_caption or cnn_rnn_caption):\n",
    "        print(\"\\nðŸ”„ Humanizing captions for better readability...\")\n",
    "        try:\n",
    "            humanized_transformer, humanized_cnn_rnn = humanize_both_captions(\n",
    "                transformer_caption, cnn_rnn_caption\n",
    "            )\n",
    "            \n",
    "            # Update global variables\n",
    "            if '_last_transformer_caption' in globals():\n",
    "                globals()['_last_transformer_caption'] = humanized_transformer\n",
    "            if '_last_cnn_caption' in globals():\n",
    "                globals()['_last_cnn_caption'] = humanized_cnn_rnn\n",
    "            \n",
    "            # Display humanized captions prominently\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"âœ¨ HUMANIZED CAPTIONS (Improved Readability):\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # Always show humanized captions, even if they're the same\n",
    "            if humanized_transformer:\n",
    "                print(f\"ðŸ“ HUMANIZED TRANSFORMER: {humanized_transformer}\")\n",
    "                if humanized_transformer != transformer_caption:\n",
    "                    print(f\"   (Original: {transformer_caption})\")\n",
    "            if humanized_cnn_rnn:\n",
    "                print(f\"ðŸ“ HUMANIZED CNN-RNN    : {humanized_cnn_rnn}\")\n",
    "                if humanized_cnn_rnn != cnn_rnn_caption:\n",
    "                    print(f\"   (Original: {cnn_rnn_caption})\")\n",
    "            \n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            # Return humanized captions\n",
    "            return image, humanized_transformer, humanized_cnn_rnn\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during humanization: {e}\")\n",
    "            print(\"Returning original captions...\")\n",
    "            return image, transformer_caption, cnn_rnn_caption\n",
    "    \n",
    "    return image, transformer_caption, cnn_rnn_caption\n",
    "\n",
    "# Replace the original function in globals (only if we have the original stored)\n",
    "if '_gpt2_original_generate_both_captions' in globals() and globals()['_gpt2_original_generate_both_captions'] is not None:\n",
    "    # Check if already wrapped to avoid re-wrapping\n",
    "    current_func = globals().get('generate_both_captions', None)\n",
    "    if current_func is None or (hasattr(current_func, '__name__') and current_func.__name__ != 'generate_both_captions_with_humanization'):\n",
    "        globals()['generate_both_captions'] = generate_both_captions_with_humanization\n",
    "        print(\"âœ… Caption humanization integrated with caption generation!\")\n",
    "        print(\"All future calls to generate_both_captions will automatically humanize the output.\")\n",
    "    else:\n",
    "        print(\"âœ… Function already wrapped with caption humanization!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Cannot wrap function: original function not found. Please run the upload widget cell first.\")\n",
    "\n",
    "# Test function to directly humanize captions and see the output\n",
    "def test_humanize_captions(transformer_caption=None, cnn_rnn_caption=None):\n",
    "    \"\"\"\n",
    "    Test function to humanize captions and display the results\n",
    "    \n",
    "    Usage:\n",
    "        test_humanize_captions(\"a man is sitting at a table\", \"person sitting at table\")\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ§ª TESTING CAPTION HUMANIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Use provided captions or get from globals\n",
    "    if transformer_caption is None and '_last_transformer_caption' in globals():\n",
    "        transformer_caption = globals()['_last_transformer_caption']\n",
    "    if cnn_rnn_caption is None and '_last_cnn_caption' in globals():\n",
    "        cnn_rnn_caption = globals()['_last_cnn_caption']\n",
    "    \n",
    "    if not transformer_caption and not cnn_rnn_caption:\n",
    "        print(\"No captions provided. Please provide captions or upload an image first.\")\n",
    "        print(\"Usage: test_humanize_captions('caption 1', 'caption 2')\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nðŸ“ ORIGINAL CAPTIONS:\")\n",
    "    print(\"-\" * 70)\n",
    "    if transformer_caption:\n",
    "        print(f\"Transformer: {transformer_caption}\")\n",
    "    if cnn_rnn_caption:\n",
    "        print(f\"CNN-RNN    : {cnn_rnn_caption}\")\n",
    "    \n",
    "    print(\"\\nðŸ”„ Humanizing for better readability...\")\n",
    "    humanized_transformer, humanized_cnn_rnn = humanize_both_captions(\n",
    "        transformer_caption, cnn_rnn_caption\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ¨ HUMANIZED CAPTIONS:\")\n",
    "    print(\"-\" * 70)\n",
    "    if humanized_transformer:\n",
    "        print(f\"Transformer: {humanized_transformer}\")\n",
    "        if humanized_transformer != transformer_caption:\n",
    "            print(\"  âœ… Changed!\")\n",
    "        else:\n",
    "            print(\"  âš ï¸  No change (same as original)\")\n",
    "    if humanized_cnn_rnn:\n",
    "        print(f\"CNN-RNN    : {humanized_cnn_rnn}\")\n",
    "        if humanized_cnn_rnn != cnn_rnn_caption:\n",
    "            print(\"  âœ… Changed!\")\n",
    "        else:\n",
    "            print(\"  âš ï¸  No change (same as original)\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return humanized_transformer, humanized_cnn_rnn\n",
    "\n",
    "print(\"Caption humanization functions ready!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Œ IMPORTANT: Execution Order\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. First, run the upload widget cell (cell with 'Interactive image upload widget')\")\n",
    "print(\"2. Then, run this cell to enable humanization\")\n",
    "print(\"3. After that, all uploaded images will have their captions automatically humanized!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ TIP: To test humanization directly, use:\")\n",
    "print(\"   test_humanize_captions('your caption here', 'another caption')\")\n",
    "print(\"   Or after uploading an image:\")\n",
    "print(\"   test_humanize_captions()  # Uses last uploaded captions\")\n",
    "print(\"\\nâœ¨ The humanization improves readability while preserving the original meaning.\")\n",
    "print(\"\\nIf you see a recursion error, please:\")\n",
    "print(\"  - Restart the kernel (Kernel â†’ Restart Kernel)\")\n",
    "print(\"  - Run all cells in order from the beginning\")\n",
    "print(\"\\nIf you've already run the upload widget cell, the wrapper is now active.\")\n",
    "print(\"Try uploading an image to see the humanized captions!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_humanize_captions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
